(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{453:function(t,a,e){"use strict";e.r(a);var r=e(10),s=Object(r.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"基本要求"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基本要求"}},[t._v("#")]),t._v(" 基本要求")]),t._v(" "),e("p",[t._v("题目：非冯诺依曼结构下的神经网络构建/神经形态计算/物理神经网络/Photonic Neural Network")]),t._v(" "),e("p",[t._v("时长：20min")]),t._v(" "),e("p",[t._v("汇报日期：2022.10.17")]),t._v(" "),e("h2",{attrs:{id:"研究背景"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#研究背景"}},[t._v("#")]),t._v(" 研究背景")]),t._v(" "),e("h3",{attrs:{id:"深度学习基本流程（p1）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#深度学习基本流程（p1）"}},[t._v("#")]),t._v(" 深度学习基本流程（P1）")]),t._v(" "),e("p",[t._v("数据预处理——信号前向传播——误差反向传播——参数更新")]),t._v(" "),e("p",[t._v("参考文献：")]),t._v(" "),e("h3",{attrs:{id:"传统网络结构（p2）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#传统网络结构（p2）"}},[t._v("#")]),t._v(" 传统网络结构（P2）")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("DNN")])]),t._v(" "),e("li",[e("p",[t._v("RNN（时序信号处理：语音、文本）")])]),t._v(" "),e("li",[e("p",[t._v("CNN（图像处理）")])]),t._v(" "),e("li",[e("p",[t._v("GAN（生成对抗网络）")]),t._v(" "),e("p",[t._v('其核心思想来自于博弈论的"纳什均衡“。它包含两个网络模型：一个生成模型和一个判别模型。生成模型捕捉样本数据的分布，判别模型是一个二分类的分类器。生成模型接受一个随机的噪声，结合学习到的样本数据特征，生成一个新的数据，交由分类模型去判断是否是“真实的”，在训练过程中，生成模型尽量生成新数据去欺骗判断模型，判断模型会尽量去识别出不真实的数据，两者实际上是一个“二元极小极大博弈问题”。最终得到一个生成模型可以从来生成新的数据。')])])]),t._v(" "),e("p",[t._v("参考资料：")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://blog.csdn.net/weixin_39653948/article/details/105170687",target:"_blank",rel:"noopener noreferrer"}},[t._v("神经网络算法详解 05：其他神经网络简介（DNN、CNN、RNN、DBN、GAN等）_datamonday的博客-CSDN博客_神经网络算法种类"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://pub.towardsai.net/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e",target:"_blank",rel:"noopener noreferrer"}},[t._v("Main Types of Neural Networks and its Applications — Tutorial | Towards AI"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464",target:"_blank",rel:"noopener noreferrer"}},[t._v("The mostly complete chart of Neural Networks, explained | by Andrew Tch | Towards Data Science"),e("OutboundLink")],1)])]),t._v(" "),e("h3",{attrs:{id:"冯诺依曼瓶颈（p3）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#冯诺依曼瓶颈（p3）"}},[t._v("#")]),t._v(" 冯诺依曼瓶颈（P3）")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012130518.png",alt:""}})]),t._v(" "),e("p",[t._v("超级计算机在物理上分离了核心内存和处理单元，这会减缓他们的速度，并大幅增加他们的能源消耗。这种“冯·诺依曼瓶颈”是人工智能算法的一个问题，它要求在每一步读取大量数据，对这些数据执行复杂操作，然后将结果写回内存24。这种数据传输过程减慢了计算速度，大大增加了学习和推理的能耗。")]),t._v(" "),e("p",[t._v("然而，它们运行的硬件，如图形处理单元或张量处理单元，限制了它们在大型、特别是能源密集型数据中心之外的开发。因此，开发更适合运行当前神经网络的硬件是一个重要的挑战。")]),t._v(" "),e("h3",{attrs:{id:"非冯诺依曼计算平台（p4）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#非冯诺依曼计算平台（p4）"}},[t._v("#")]),t._v(" 非冯诺依曼计算平台（P4）")]),t._v(" "),e("p",[t._v("耗能：通俗比方，蠕虫和计算机")]),t._v(" "),e("p",[t._v("第一种方法是将目前在人工智能中使用的传统神经网络算法映射到专用物理系统，以提高其实现的能效。第二种方法是超越这些算法，从神经科学中获得灵感，为人工神经网络配备额外的功能和动力学，希望实现更复杂的计算。")]),t._v(" "),e("p",[t._v("【插入常见非冯诺依曼计算平台的应用】")]),t._v(" "),e("p",[t._v("说明：侧重网络设计，后面以光学为例")]),t._v(" "),e("h2",{attrs:{id:"网络复刻（p5）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#网络复刻（p5）"}},[t._v("#")]),t._v(" 网络复刻（P5）")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("CMOS+忆阻器")]),t._v(" "),e("th",[t._v("全光学系统/光电混合系统")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012095101.png",alt:""}})]),t._v(" "),e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012094825.png",alt:""}})])]),t._v(" "),e("tr",[e("td",[t._v("在混合CMOS/记忆系统中，神经元由模拟或数字CMOS组成，流经网络的信息是电流。因此，为了调节信息流，突触应该充当电流的阀门。这种阀门可以用记忆电阻器实现（“记忆电阻”的缩写，也称为电阻开关器件）。")]),t._v(" "),e("td")])])]),t._v(" "),e("p",[t._v("问题：")]),t._v(" "),e("ul",[e("li",[t._v("在传统计算机上训练好后移植过来，精度不够？")]),t._v(" "),e("li",[t._v("元器件太多，需要精密调谐")])]),t._v(" "),e("h2",{attrs:{id:"类脑：spiking-neural-network（p10）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#类脑：spiking-neural-network（p10）"}},[t._v("#")]),t._v(" 类脑：Spiking Neural Network（P10）")]),t._v(" "),e("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012205824.png"}}),t._v(" "),e("p",[t._v("主要讲")]),t._v(" "),e("p",[e("strong",[t._v("为什么脉冲神经网络在非冯诺依曼结构上有广泛应用？")])]),t._v(" "),e("h2",{attrs:{id:"折叠：延迟动力系统"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#折叠：延迟动力系统"}},[t._v("#")]),t._v(" 折叠：延迟动力系统")]),t._v(" "),e("h3",{attrs:{id:"reservoir-computing：环状拓扑"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#reservoir-computing：环状拓扑"}},[t._v("#")]),t._v(" Reservoir Computing：环状拓扑")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("基本原理（P6）")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Reservoir Computing")]),t._v(" "),e("th",[t._v("Time-delayed Reservoir Computing")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221011144852.png",alt:"img"}})]),t._v(" "),e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221008095806.png",alt:"img"}})])])])]),t._v(" "),e("p",[t._v("参考文献：《Information processing using a single dynamical node as complex system》")])]),t._v(" "),e("li",[e("p",[t._v("网络变种：Deep Time-delayed Reservoir Computing")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221011150701.png",alt:""}})]),t._v(" "),e("p",[t._v("参考文献：《Deep time-delay reservoir computing: Dynamics and memory capacity》")])]),t._v(" "),e("li",[e("p",[t._v("应用与分析（P7）")])])]),t._v(" "),e("h3",{attrs:{id:"任意拓扑（p8）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#任意拓扑（p8）"}},[t._v("#")]),t._v(" 任意拓扑（P8）")]),t._v(" "),e("p",[t._v("参考文献：《Experiments with arbitrary networks in time-multiplexed delay systems》")]),t._v(" "),e("p",[t._v("问题：隐藏层不可训练，反向传播是大规模网络训练的必要手段")]),t._v(" "),e("h3",{attrs:{id:"folded-in-time-dnn（p9）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#folded-in-time-dnn（p9）"}},[t._v("#")]),t._v(" Folded-in-time DNN（P9）")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("基本原理")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("折叠形态")]),t._v(" "),e("th",[t._v("展开形态")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[e("img",{attrs:{src:"https://i.loli.net/2021/09/25/91p26iCtvTGBEL8.png",alt:"Fit-DNN框架"}})]),t._v(" "),e("td",[e("img",{attrs:{src:"https://i.loli.net/2021/09/25/48io1Pd6n2mrLHX.png",alt:"img"}})])])])]),t._v(" "),e("p",[t._v("参考文献：《Deep neural networks using a single neuron: folded-in-time architecture using feedback-modulated delay loops》")])]),t._v(" "),e("li")]),t._v(" "),e("p",[t._v("过渡：因为前向传播是物理过程，无法在原位进行反向传播")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012153409.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"支持原位训练：physics-awared-training（p11）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#支持原位训练：physics-awared-training（p11）"}},[t._v("#")]),t._v(" 支持原位训练：Physics-awared Training（P11）")]),t._v(" "),e("p",[t._v("将反向传播应用于训练可控物理系统。正如深度学习使用由数学函数层构成的深度神经网络实现计算一样，我们的方法允许我们训练由可控物理系统层构成的深层物理神经网络，即使物理层与传统人工神经网络层缺乏任何数学同构。\n为了证明我们方法的普遍性，我们训练了基于光学、力学和电子学的各种物理神经网络，以实验方式执行音频和图像分类任务。物理感知训练结合了反向传播的可伸缩性和原位算法可实现的缺陷和噪声的自动缓解。物理神经网络有潜力比传统电子处理器更快、更节能地执行机器学习，更广泛地说，它可以赋予物理系统自动设计的物理功能，例如机器人23–26、材料27–29和智能传感器30–32。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012153312.png",alt:""}})]),t._v(" "),e("p",[t._v("如何训练？")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012185105.png",alt:""}})]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("验证模型训练的有效性")]),t._v(" "),e("th",[t._v("验证PAT训练方法的有效性")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012193253.png",alt:""}})]),t._v(" "),e("td",[e("img",{attrs:{src:"https://fastly.jsdelivr.net/gh/ZhouWenjun2019/images/20221012193154.png",alt:""}})])])])]),t._v(" "),e("p",[t._v("参考文献：")]),t._v(" "),e("ul",[e("li",[t._v("《Deep physical neural networks trained with backpropagation》")]),t._v(" "),e("li",[e("a",{attrs:{href:"https://www.bilibili.com/video/av211954256/?vd_source=243a2ef5f32d9781a2177d603e71baf4",target:"_blank",rel:"noopener noreferrer"}},[t._v("nature-文献阅读-深度学习新方法PNN（反向传播训练的深层物理神经网络）_哔哩哔哩_bilibili"),e("OutboundLink")],1)])]),t._v(" "),e("h2",{attrs:{id:"前景与挑战（p12）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#前景与挑战（p12）"}},[t._v("#")]),t._v(" 前景与挑战（P12）")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("算法层面")]),t._v(" "),e("ul",[e("li",[t._v("前后向传播不一致：前向物理过程不能解析微分，只能近似估计，用数值模拟训练物理系统，造成模拟-现实差距【层数越深越明显】")]),t._v(" "),e("li")])]),t._v(" "),e("li",[e("p",[t._v("大规模硬件部署")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("精度问题：大多数纳米器件本身就有噪声，其有效重量值不会完全连续变化")])]),t._v(" "),e("li",[e("p",[t._v("首先，在神经形态计算中，纳米突触和纳米神经元之间的紧密高效互连是一个很大的困难。")])]),t._v(" "),e("li",[e("p",[t._v("非线性运算：物理元器件的非线性函数不是那么理想化的")])]),t._v(" "),e("li",[e("p",[t._v("迁移导致精度降低")])])])])]),t._v(" "),e("h2",{attrs:{id:"疑问"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#疑问"}},[t._v("#")]),t._v(" 疑问")]),t._v(" "),e("ul",[e("li",[t._v("如果SNN是非冯诺依曼结构的基本算法，那么RC之类的算什么")])]),t._v(" "),e("h2",{attrs:{id:"参考文献"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考文献"}},[t._v("#")]),t._v(" 参考文献")]),t._v(" "),e("ul",[e("li",[t._v("《Physics for neuromorphic computing》")])])])}),[],!1,null,null,null);a.default=s.exports}}]);